{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample background \n",
    "\n",
    "Individual guano samples were processed in two separate sequencing runs. See Jupyter notebooks titled \"bri-P1-notes\" and \"bri-P2-notes\" for initial processing details. This notebook functions by beginning with concatenating individually completed sequencing fastq files, removing any required reads associated for samples within those files we want to discard, then continuing with the clustering of all reads without the mock community. Index-bleed filters and auto-subtract values are assigned to this common pool of clustered reads using the more conservative when applicable. See below for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running core programs:\n",
    "    - AMPtk 0.8.6\n",
    "    - vsearch v2.3.4_linux_x86_64\n",
    "    - usearch usearch v9.2.64_i86linux32\n",
    "    - python 2.7.6\n",
    "- Installed the necessary (COI) databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cleanups before OTU clustering\n",
    "\n",
    "#### Concatenating\n",
    "The samples from Project1 and Project2 were combined by concatenating the two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to concatenate two {}.demux.fq files\n",
    "cat \\\n",
    "/leo/devon/projects/guano/bri2016/p1_data/bri-chunk1/cleaned.demux.fq \\\n",
    "/leo/devon/projects/guano/bri2016/p2_data/bridropd.demux.fq > prebriall.demux.fq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample read removal\n",
    "Initially when completing the analyses on the individual sequencing runs, samples in Project1 with more than 5,000 reads were kept while samples in Project2 were kept with 10,000 reads (for various reasons including NTC read depth, mock community contamination, etc.) I wanted to be unifrom as possible when combining the two datasets and went with a more conservative approach by requiring 10,000 reads for any sample in this project. Therefore, 28 additional samples from Project1 were dropped, as listed in the file **morefiles2drop.txt**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk remove -i prebriall.demux.fq -f morefiles2drop.txt -o briall.demux.fq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of samples following concatenation between both projects is **210 samples**. We are highly confident that these samples contain a meaningful number of reads to ensure that our community analyses are robust, while also containing as little type-1 error as possible in the forthcoming OTU table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2 - OTU clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to cluster P1L1 data:\n",
    "amptk dada2 \\\n",
    "--fastq briall.demux.fq \\\n",
    "--out briall \\\n",
    "--length 180 \\\n",
    "--platform illumina \\\n",
    "--uchime_ref COI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the output using the default settings with DADA2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[10:06:33 AM]: 13,151,134 reads passed\n",
    "[11:46:51 AM]: 7,671 total inferred sequences (iSeqs)\n",
    "[11:46:51 AM]: 5,036 denovo chimeras removed\n",
    "[11:46:51 AM]: 2,635 valid iSeqs\n",
    "[11:47:04 AM]: 2,537 iSeqs passed, 98 ref chimeras removed\n",
    "[12:34:15 PM]: 13,059,451 reads mapped to iSeqs (98%)\n",
    "[12:34:17 PM]: 1,662 OTUs generated\n",
    "[01:14:46 PM]: 13,017,444 reads mapped to OTUs (98%)\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is to filter the OTU table with the fasta file listed above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: filtering OTU table\n",
    "\n",
    "Note that there is something a bit unique about this analysis: the two independent sequencing runs (P1 and P2) each had their own mock communities; normally I'd take the more conservative estimate from either run and then apply that here, but there were some subtleties that required keeping the mock community in this dataset. These are going to be analyzed now to determine a few factors:  \n",
    "\n",
    "1. What's the **index-bleed** threshold we want to apply? \n",
    "    Earlier work from the independent runs suggested somewhere between 0.1 - 0.3%\n",
    "\n",
    "2. What's the **subtract** threshold we want to apply?\n",
    "    Independent sequencing runs reported values of 47 and 86; thus the more conservative measure would be 86. However, this higher value (from Project 1 sequencing run) allowed for three non-intended OTUs to remain in the dataset in low values; it's unclear whether these are the result of index-bleed from actual *BRI samples*, or from another sample placed on the same P1 lane (for example, from O'Rourke samples, or others), or if it's from low-level contamination that was incorporated at the PCR step earlier in the workflow.  \n",
    "\n",
    "3. How many OTUs will be dropped if necessary to balance the **subtract** value with overall OTU retention?  \n",
    "\n",
    "To address these questions we'll run preliminary filtering steps and assess their output, as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk filter \\\n",
    "-i briall.cluster.otu_table.txt \\\n",
    "-f briall.cluster.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--keep_mock \\\n",
    "--calculate in \\\n",
    "--mc /leo/devon/projects/guano/mock-fa/CFMR_insect_mock3.fa \\\n",
    "--debug \\\n",
    "--subtract auto \\\n",
    "-o testfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the relevant output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[01:32:01 PM]: OTU table contains 1662 OTUs\n",
    "[01:32:03 PM]: Index bleed, samples into mock: 1.405000%.\n",
    "[01:32:03 PM]: Auto subtract filter set to 977\n",
    "[01:32:09 PM]: Filtering OTU table down to 406 OTUs\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we have a pretty high index bleed (but not awful), but a HUGE **subtract** value. From earlier work on each sequencing runs (P1 and P2), this is likely due to a handful of samples. To address which samples contain the highest number of reads, see the basic notes outlined in \"bri-P2-notes\". Bioinformatic steps and subsequent results are briefly explained below.  \n",
    "\n",
    "First, identifying the number of contaminant OTUs in our combined mock community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sed -i 's/#OTU ID/OTUid/' testfilt.final.txt\n",
    "sed -i 's/#OTU ID/OTUid/' testfilt.normalized.num.txt\n",
    "awk -F '\\t' '{print NF; exit}' testfilt.final.txt\n",
    "    #212\n",
    "\n",
    "cut testfilt.normalized.num.txt -f 1,211 | sort -k2,2n | awk '$2 != \"0.0\" {print $0}'\n",
    "    #OTU249 is the culprit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script produces a long list (see below) of all OTUs present in our mock community, though nearly all of these unwanted OTUs are in very low abundance (while, importantly, all the OTUs that should be in our mock community are there in relatively high numbers). Note, however, one OTU (OTU249) contains a much higher number of reads than any other unwanted OTU. This is in one sense reassuring, as it implies that the high **subtract** value found in our first-pass filtering step is due to just a single OTU, and by dropping just that one OTU we sould be able to retain far more reads/OTUs when our final filter is applied.  \n",
    "\n",
    "- ...*(more values in list containing OTUs with fewer than 36.0 reads)*...\n",
    "- OTU497\t36.0\n",
    "- OTU151\t41.0\n",
    "- OTU21\t48.0\n",
    "- MockIM34_pident=100.0_OTU349\t682.0\n",
    "- **OTU249\t977.0**\n",
    "- MockIM49_pident=100.0_OTU207\t3960.0\n",
    "- MockIM6_pident=100.0_OTU188\t4976.0\n",
    "- MockIM15_pident=99.4_OTU161\t7036.0  \n",
    "\n",
    "The next question is whether or not **OTU249** persists among many samples in addition to our mock community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grep \"\\\\bOTU249\\\\b\" testfilt.normalized.num.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result indicates that OTU249 is highest in our mock community (recall it's value was **977** reads); only 14 other samples have more than 100 reads (most are 0.0). Average was ~31, but deviation was about 119. See **otu249counts.txt** for table of these counts. I further investigated whether or not there was any relationship for a particular factor (index, PCR or DNA plate/well, etc.) associated with contamination; there doesn't appear to be one among the most contaminated. See **otu249contamtable.txt** for these results.  \n",
    "\n",
    "To ensure this OTU is likely the same contaminant observed in individual analyses performed earlier, a BLAST search was performed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grep \"\\\\bOTU249\\\\b\" briall.cluster.otus.fa -A 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alignment results indicate it's Fannia canicularis, the Lesser housefly, which was what was observed from earlier independent analyses for the P1 data set. It's unclear whether contamination was from index bleed from other samples not part of this dataset (ex. other bird sequencing projects), or from primer contamination. Regardless, dropping this one OTU is likely of little consequence to broader interpretation as it is not particularly common in our dataset.  \n",
    "\n",
    "To remove OTU249:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk drop \\\n",
    "-i briall.cluster.otus.fa \\\n",
    "-r briall.demux.fq \\\n",
    "-l OTU249 \\\n",
    "-o bri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above steps likely address the problem of OTU contamination in our mock community. However, there is a single negative control in this dataset that should be dropped - sample \"**negbri-41C03**\". This sample wasn't removed initially because it's useful to notice how many OTUs are present, and to what degree (in terms of relative/normalized reads). In certain cases where negative controls show some appreciable number of amidst true samples, we find that a single OTU or two should be dropped from all samples (like above). However, this sample doesn't appear to be a typical cross-contamination event; rather, I think this sample represents one of two scenarios:  \n",
    "- (a) A guano sample placed in an adjacent well split and fell into this negative control well, and thus represents a pseudoreplicate.\n",
    "- (b) A guano sample was misplaced directly into the negative control well, and thus represents a true sample that is simply mislabeled.  \n",
    "\n",
    "A quick analysis of this sample suggests the following:  \n",
    "1. There is a single OTU that contains the majority of the reads associated with this sample: OTU2 (see file **negconcounts.txt** for all OTU counts associated with this sample). This OTU exists in just a handful of all true samples, with only 7 samples containing >100 reads (though 5 samples contain >10,000 reads). Thus this OTU best aligns to one of two *crane flys*, which are fairly large and may take up a significant proportion of a single guano sample.\n",
    "2. There are just 5 other OTUs that contain >100 reads:\n",
    "    - OTU48, OTU154, OTU34 follow the similar pattern above: only a few samples contain many reads associated with this particular OTU, and there's a huge variance in total number of reads. \n",
    "    - OTU48 is another crane fly; OTU154 is a caddisfly; OTU34 is a northeastern moth; OTU150 is a leafroller moth. In other words, this sample contains insects we'd expect in a piece of bat guano (so you can rule out contamination from other guano, say bird guano, from other projects).\n",
    "3. I created another file, **contamprobs.txt** that looks at these top 6 OTUs for all samples. It doesn't fit any single other sample perfectly, but it matches to loads of other samples qualitatively. In other words, plenty of samples contain some number of reads for these OTUs in question, but no single sample has at least 100 reads for every one of these OTUs.  \n",
    "\n",
    "Observation #3 above strongly suggests this being a true sample, though it's unclear whether its a case of a guano sample splitting into the wrong well (as in '**a**') or a true sample that was mislabeled (as in '**b**'). Dropping the OTUs associated with this sample is likely unnecessary. Rather, dropping this sample from further analysis is what makes more sense, as we'd like to retain these OTUs that are present in the data.  \n",
    "\n",
    "This will be done post-taxonomic classification in R, however, if this information was available a priori we could have dropped it earlier with the ** *amptk remove* ** step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll re-run our filtering program this time without that contaminant OTU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk filter \\\n",
    "-i bri.cleaned.otu_table.txt \\\n",
    "-f bri.cleaned.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--keep_mock \\\n",
    "--calculate in \\\n",
    "--mc /leo/devon/projects/guano/mock-fa/CFMR_insect_mock3.fa \\\n",
    "--debug \\\n",
    "--subtract auto \\\n",
    "-o testfilt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[03:20:07 PM]: OTU table contains 1661 OTUs\n",
    "[03:20:08 PM]: Will use value of 0.500000% for index-bleed OTU filtering.\n",
    "[03:20:08 PM]: Subtracting 48 from OTU table\n",
    "[03:20:19 PM]: Filtering OTU table down to 984 OTUs\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results indicate that our **index-bleed** can be set at **0.05%** (a typical value) and the auto-subtract value can be set to **48**. Note that while this is less of a conservative of a subtract value for one of the two runs (P2 was **47** while P1 was **86**), the fact that we summed up *three* mock communities is potentially artificially inflating our overall counts for any OTU associated with a mock community (despite that fact that we're normalizing reads when making these calculations).\n",
    "\n",
    "Notably, we've increased the number of OTUs retained from our initial filtering (default) parameters when including OTU249. By dropping that single OTU from the dataset we've reduced the **subtract** value by about 900, thus we've recovered the number of OTUs by more than double (we kept **984 OTUs** rather than the earlier **406**).  \n",
    "\n",
    "A final filtering step is applied whereby the mock community reads are removed leaving us with just true samples (and that unknown sample masquerading as a negative control sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk filter \\\n",
    "-i bri.cleaned.otu_table.txt \\\n",
    "-f bri.cleaned.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--calculate in \\\n",
    "--mc /leo/devon/projects/guano/mock-fa/CFMR_insect_mock3.fa \\\n",
    "--subtract auto \\\n",
    "-o bri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments above will produce the following files:  \n",
    "- **filtd.filtered.otus.fa** is the final filtered fasta file (say that five times fast)\n",
    "- **filtd.final.binary.txt** is the presence/absence OTU table after filtering\n",
    "- **filtd.stats.txt** is the number of OTUs in each sample before and after filtering\n",
    "- **filtd.final.txt** is the OTU table with normalized read counts (noramlized to the number of reads in each sample)\n",
    "- **filtd.amptk-filter.log**  \n",
    "\n",
    "The next step is to assign taxonomy by using the **bri.filtered.otus.fa** and **bri.final.binary.txt** files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Assigning Taxonomy to OTUs\n",
    "\n",
    "Make sure to have already downloaded the necessary database (COI). Need to run this command for every instance in which you've generated a uniquely filtered dataset from Parts 1-3. Here's just a pair of examples you'd run in their respective directories:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "amptk taxonomy -i bri.final.binary.txt -f bri.filtered.otus.fa -d COI -o bri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OTU classification is performed with UTAX and SINTAX in addition to UPARSE; all rely to some degree on the custom database acquired through BOLD; see J. Palmer for details about its creation).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we find?\n",
    "\n",
    "See the R script 'ufits_2016_OTUtable_analysis.R' for generating the following observations.  \n",
    "\n",
    "Move the files as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#note the files will be copied to the current directory:\n",
    "rsync devon@pinky.sr.unh.edu:/leo/devon/projects/guano/bri2016/catP1P2/bri.otus.taxonomy.fa .\n",
    "\n",
    "##as well as these files:\n",
    "rsync devon@pinky.sr.unh.edu:/leo/devon/projects/guano/bri2016/catP1P2/bri.taxonomy.txt .\n",
    "rsync devon@pinky.sr.unh.edu:/leo/devon/projects/guano/bri2016/catP1P2/bri.otu_table.taxonomy.txt .\n",
    "rsync devon@pinky.sr.unh.edu:/leo/devon/projects/guano/bri2016/catP1P2/bri.filtered.otus.fa ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLASTing some of those unknowns \n",
    "\n",
    "We can use the following scripts to run NCBI BLAST. This will happen in three steps:\n",
    "\n",
    "- A. Clean up the file to pull out just the UTAX-tagged OTUs and convert into a single-line fasta\n",
    "- B. Build a blast database (* note we're not going to use this step at the moment*)\n",
    "- C. BLAST the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.** First, use fastx toolkit to convert '{name}.otus.taxonomy.fa' into oneliner fasta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fasta_formatter -i D2bri-chunk1.otus.taxonomy.fa -o allOTUs_oneliner.fasta -w 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then grep only lines with \"UTAX\" or \"SINTAX\" in header and remove the dashes where the lines were removed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "egrep '(UTAX|SINTAX)' allOTUs_oneliner.fasta -A 1 | sed '/^-/d' > blastit.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** We're going to skip the step of setting up a BLAST database for the moment - there's already the complete 'nr' database installed on Pinky, so unless we want to set up a custom database with specific sequences we don't have available through NCBI, then I wouldn't worry about this step. If you did want to do it, you'd run the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "makeblastdb -in {blast_subjects.fasta} -parse_seqids -dbtype nucl -out {path.to}/my_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.** Finally, we're going to run a BLAST search on the UTAX and SINTAX sequences using NCBI's nr database. We're going to specify a few other flags described in the code below. Specifically, we're going to take only values that are a certain alignment length, a certain percent identity, and then filter these to choose the best possible taxonomic description to those collapsed reads.  \n",
    "\n",
    "See [here] [link1] for BLAST manual with command line options used below.\n",
    "[link1]:http://www.ncbi.nlm.nih.gov/books/NBK279675/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "blastn \\\n",
    "-query /leo/devon/projects/bri2016/bri-chunk1/blast_out/blastit.fa \\\n",
    "-db /opt/ncbi-blast-2.2.29+/db/nt \\\n",
    "-outfmt '6 qseqid sseqid pident length bitscore evalue staxids' \\\n",
    "-num_threads 8 \\\n",
    "-perc_identity 79.9 \\\n",
    "-max_target_seqs 12 \\\n",
    "-out /leo/devon/projects/bri2016/bri-chunk1/blast_out/blastout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the taxonomy information from these taxids we're going to use an R package called [taxize] [link1] to convert the *staxids* value into taxonomic information (kingdom, phylum,...species).  The only information we need to provide to **taxize** is a list of the taxids.  \n",
    "\n",
    "However, we're going to sort through our blast output a little bit first.\n",
    "\n",
    "[link1]:https://github.com/ropensci/taxize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. because I had included a blast result column with taxonmy names that didn't work and printed \"N/A\" I removed it and created tab-separated fields as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "awk '{print $1,'\\t',$2,'\\t',$3,'\\t',$4,'\\t',$5,'\\t',$6,'\\t',$7}' blastout.txt > clnblastout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. I then removed redundant rows that contained identical OTU id's, bit scores, and taxIDs. I also then removed any rows in which the alignment length of a match was less than 150 base pairs:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sort -u -k1,1 -k5,5nr -k7,7 clnblastout.txt | awk '$4 > 149 {print $0}' > cln2blastout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. I then filtered out any redundant row with common OTUid and TAXid, then sorted by bit score (highest first)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sort -u -k1,1 -k7,7 cln2blastout.txt | sort -k1,1 -k5,5nr > cln3blastout.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in filtering down from the initial **2295 rows** to **903 rows**. We'll use these results to investigate whether or not the taxonomy information assigned by UTAX/SINTAX could be updated or not.  \n",
    "\n",
    "The next step is to import these TAXid values into R and use a package called **taxize**.  For ease of use, I'm going to keep all the remaining information in this final file as a single dataframe, then pull that specific row in R out to make the necessary taxonomic classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## taxize R work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(taxize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare what you're trying to clean up with what's already been assigned by the original SINTAX or UTAX classifcation in the .fa file from the UFITS taxonomy output. You're going to need to get it into a format that R won't hate you for though:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "awk 'BEGIN{FS=\" |;|,\"; OFS = \"\\t\"}{print $1, $2, $3, $4, $5, $6, $7, $8, $9}' taxheadsonly.txt > clntaxheads.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out this link for details about why we're filtering how we are... https://edwards.sdsu.edu/research/percent-similarity-at-different-taxonomic-levels/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#let's fix the stupid names...\n",
    "\n",
    "#use Excel to convert what you want... then update the OTUs you want manually in a text file with vim\n",
    "#to generate the file to modify...\n",
    "cut -f 1,182 D2bri-chunk1.otu_table.taxonomy.txt > OTUs_tomodify.txt\n",
    "##then modify this file with Excel\n",
    "\n",
    "#remove the last field of the main file you're going to replace it with\n",
    "rev D2bri-chunk1.otu_table.taxonomy.txt | cut -f2- | rev > stack1.txt\n",
    "\n",
    "#remove the last field of the modified file\n",
    "cut -f2 OTUs_tomodify.txt > stack2.txt\n",
    "\n",
    "#replace with the correct names by pasting the two files together\n",
    "paste stack1.txt stack2.txt > modOTU_table.taxonomy.txt\n",
    "\n",
    "...... alternatively, we could update COI's database as follows:\n",
    "\n",
    "#create file with OTUs to search for ==> \"searchfile.txt\"\n",
    "OTU100\n",
    "OTU1034\n",
    "OTU113\n",
    "... 43 more ...\n",
    "OTU934\n",
    "\n",
    "#search in one-line fasta for these terms, and get rid of hyphen separating returned values:\n",
    "grep -Fwf ../greptest2.txt allOTUs_oneliner.fasta | sed '/^-/d' > fastatoFix.fa\n",
    "\n",
    "\n",
    "#convert fasta file to tab-delimited text file, and get just the second column of this file (the sequences):\n",
    "awk 'BEGIN{RS=\">\"}NR>1{sub(\"\\n\",\"\\t\"); gsub(\"\\n\",\"\"); print RS$0}' fasta_toFix.fa | cut -f 2 > paste2.txt\n",
    "\n",
    "\n",
    "#create file with replacement classification scheme, created using blast output and excel, concatenating taxonomic information...\n",
    ">ncbi:258930;tax=k:Animalia,p:Chordata,c:Mammalia,o:Chiroptera,f:Vespertilionidae,g:Lasiurus,s:Lasiurus borealis\n",
    ">ncbi:1822957;tax=k:Animalia,p:Arthropoda,c:Insecta,o:Diptera,f:Empididae\n",
    ">ncbi:1770991;tax=k:Animalia,p:Arthropoda,c:Insecta,o:Coleoptera,f:Elateridae,g:Melanotus,s:Melanotus similis\n",
    "... 43 more ...\n",
    ">ncbi:1847630;tax=k:Animalia,p:Arthropoda,c:Insecta,o:Lepidoptera,f:Tortricidae,g:Phiaris,s:Phiaris bipunctana\n",
    "\n",
    "\n",
    "#paste the replacement taxonomic information with the sequences:\n",
    "paste paste1.txt paste2.txt > converttofasta.txt\n",
    "\n",
    "\n",
    "#create a new fasta file from that tab-delimited text file:\n",
    "awk -F '\\t' '{print $1\"\\n\"$2}'  converttofasta.txt > forfastX.fa\n",
    "\n",
    "#convert back to 60-character multiline fasta using fastx toolkit's \"FASTA Formatter\"\n",
    "fasta_formatter -i forfastX.fa -w 60 > addthese.fa\n",
    "\n",
    "#verify they look correct:\n",
    "head -n 5 addthese.fa \n",
    "    # returns\n",
    ">ncbi:258930;tax=k:Animalia,p:Chordata,c:Mammalia,o:Chiroptera,f:Vespertilionidae,g:Lasiurus,s:Lasiurus borealis\n",
    "CACTTTATATTTACTATTTGGCGCTTGAGCGGGAATAGTTGGGACTGCATTAAGCCTATT\n",
    "AATTCGAGCTGAATTAGGCCAACCAGGTGCTCTTTTAGGAGATGATCAAATCTATAACGT\n",
    "TATCGTGACTGCCCATGCATTCGTAATAATTTTTTTTATAGTCATGCCCATTATAATTGG\n",
    ">ncbi:1822957;tax=k:Animalia,p:Arthropoda,c:Insecta,o:Diptera,f:Empididae\n",
    "AACTCTCTATTTCATCTTTGGAGCATGAGCAGGAATGCTAGGAACATCTCTAAGACTCTT    \n",
    "\n",
    "## then!!! create a new ufits DB (or rather, update what we have)\n",
    "\n",
    "#step 1 - get length of original COI.extracted.fa file:\n",
    "grep -c \"^>\" COI.extracted.fa\n",
    "    #returns value of 332329\n",
    "\n",
    "#step 2 - create a new .fa file by adding these additional fasta sequences:\n",
    "    #perform in .../ufits-v0.7.2/DB/ directory\n",
    "    cat COI.extracted.fa addthese.fa > COI.updated.fa\n",
    "    #confirm it's added the right number of sequences:\n",
    "    grep -c \"^>\" COI.updated.fa\n",
    "        #returns value of 332376 - good! it's 47 longer than the original, and that's how many new records we added\n",
    "\n",
    "\n",
    "\n",
    "#step 3 - update the database in ufits with this new fasta\n",
    "ufits database -i COI.updated.fa -o COI --create_db usearch --keep_all --skip_trimming --format off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2 - redoing the analyses with the updated DB\n",
    "\n",
    "See the extensive notes above for explanations about how and why each of the following commands are executed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster again\n",
    "\n",
    "Start at Part 2 - clustering with DADA2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#running in: /leo/devon/projects/bri2016/bri-chunk1/updatedDB\n",
    "\n",
    "ufits dada2 \\\n",
    "--fastq /leo/devon/projects/bri2016/bri-chunk1/cleaned.demux.fq \\\n",
    "--uchime_ref COI \\\n",
    "-o clustR2 \\\n",
    "--length 180 \\\n",
    "--platform illumina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output differs slightly from before - more chimeras removed; otherwise pretty much identical output."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[01:05:39 PM]: OS: linux2, 32 cores, ~ 264 GB RAM. Python: 2.7.6\n",
    "[01:05:39 PM]: ufits v.0.7.3, USEARCH v9.2.64, VSEARCH v2.3.4\n",
    "[01:05:39 PM]: Loading FASTQ Records\n",
    "[01:05:42 PM]: 6,377,119 reads (2.4 GB)\n",
    "[01:06:25 PM]: Quality Filtering, expected errors < 1.0\n",
    "[01:08:41 PM]: 6,307,608 reads passed\n",
    "[01:09:09 PM]: DADA2 compatible read lengths, avg: 181 bp, min: 160 bp, max: 310 bp, top 95%: 181 bp\n",
    "[01:09:09 PM]: Splitting FASTQ file by Sample and truncating to 180 bp\n",
    "[01:12:40 PM]: Running DADA2 pipeline\n",
    "[01:35:10 PM]: R v3.3.2, DADA2 v1.3.0\n",
    "[01:35:10 PM]: 5,242 total inferred sequences (iSeqs)\n",
    "[01:35:10 PM]: 3,369 denovo chimeras removed\n",
    "[01:35:10 PM]: 1,873 valid iSeqs\n",
    "[01:35:10 PM]: Chimera Filtering (VSEARCH) using COI DB\n",
    "[01:35:17 PM]: 1,784 iSeqs passed, 89 ref chimeras removed\n",
    "[01:35:18 PM]: Mapping reads to DADA2 iSeqs\n",
    "[01:43:03 PM]: 6,213,934 reads mapped to iSeqs (97%)\n",
    "[01:43:03 PM]: Clustering iSeqs at 97% to generate biological OTUs\n",
    "[01:43:04 PM]: 1,183 OTUs generated\n",
    "[01:43:04 PM]: Mapping reads to OTUs\n",
    "[01:48:32 PM]: 6,172,191 reads mapped to OTUs (97%)\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter again\n",
    "\n",
    "Determine if OTUs in mock look similar and if similar **index-bleed** and **substract** values are warranted."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#running in: /leo/devon/projects/bri2016/bri-chunk1/updatedDB\n",
    "\n",
    "ufits filter \\\n",
    "-i clustR2.cluster.otu_table.txt \\\n",
    "-f clustR2.cluster.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--keep_mock \\\n",
    "--mc /leo/devon/projects/nhguano2016/CFMR_insect_mock3.fa \\\n",
    "--debug \\\n",
    "--subtract auto \\\n",
    "-o filt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output looks identical to previous run..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[02:01:25 PM]: OS: linux2, 32 cores, ~ 264 GB RAM. Python: 2.7.6\n",
    "[02:01:26 PM]: ufits v.0.7.3, USEARCH v9.2.64, VSEARCH v2.3.4\n",
    "[02:01:26 PM]: Loading OTU table: clustR2.cluster.otu_table.txt\n",
    "[02:01:26 PM]: OTU table contains 1183 OTUs\n",
    "[02:01:26 PM]: Mapping OTUs to Mock Community (USEARCH)\n",
    "[02:01:26 PM]: Sorting OTU table naturally\n",
    "[02:01:26 PM]: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
    "[02:01:26 PM]: Normalizing OTU table to number of reads per sample\n",
    "[02:01:26 PM]: Index bleed, samples into mock: 1.531092%.\n",
    "[02:01:26 PM]: Will use value of 1.600000% for index-bleed OTU filtering.\n",
    "[02:01:26 PM]: Auto subtract filter set to 204\n",
    "[02:01:26 PM]: Subtracting 204 from OTU table\n",
    "[02:01:27 PM]: Filtering OTU table down to 513 OTUs\n",
    "[02:01:27 PM]: Filtering valid OTUs\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate these files:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#cleanup\n",
    "sed -i 's/#OTU ID/OTUid/' filt1.final.txt\n",
    "sed -i 's/#OTU ID/OTUid/' filt1.normalized.num.txt\n",
    "\n",
    "#how many fields\n",
    "awk -F '\\t' '{print NF; exit}' filt1.final.txt\n",
    "    #confirm the last field is the mock community\n",
    "        #yep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where are the problems?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for the .normalized.num.txt file:\n",
    "cut filt1.normalized.num.txt -f 1,182 | sort -k2,2n | awk '$2 != \"0.0\" {print $0}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows the exact same number of normalized counts as the earlier data set. The third OTU listed here was further analyzed with an NCBI BLAST search from the **cluster.otus.fa** file. This confirms that OTU55 from our first clustering in the previous data set is the same sequence as OTU54 here.  \n",
    "\n",
    "In other words, renaming these new sequences to our database didn't change anything about the mock community (as expected!)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OTU8\t108.0\n",
    "OTU82\t173.0\n",
    "OTU54\t204.0\n",
    "MockIM34_pident=100.0_OTU386\t445.0\n",
    "MockIM49_pident=100.0_OTU219\t2734.0\n",
    "MockIM15_pident=99.4_OTU189\t4958.0\n",
    "MockIM6_pident=100.0_OTU190\t5001.0\n",
    "MockIM40_pident=100.0_OTU167\t7614.0\n",
    "MockIM46_pident=100.0_OTU161\t9576.0\n",
    "MockIM39_pident=100.0_OTU157\t10076.0\n",
    "MockIM41_pident=97.2_OTU155\t10746.0\n",
    "MockIM5_pident=100.0_OTU141\t12702.0\n",
    "MockIM4_pident=100.0_OTU127\t15324.0\n",
    "MockIM21_pident=97.8_OTU154\t19287.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to apply the exact same filtering parameters as before, given our mock community looks identical:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ufits filter \\\n",
    "-i clustR2.cluster.otu_table.txt \\\n",
    "-f clustR2.cluster.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--mc /leo/devon/projects/nhguano2016/CFMR_insect_mock3.fa \\\n",
    "--subtract 86 \\\n",
    "--delimiter tsv \\\n",
    "-o filtd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get identical outputs as before:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[02:10:50 PM]: OS: linux2, 32 cores, ~ 264 GB RAM. Python: 2.7.6\n",
    "[02:10:50 PM]: ufits v.0.7.3, USEARCH v9.2.64, VSEARCH v2.3.4\n",
    "[02:10:50 PM]: Loading OTU table: clustR2.cluster.otu_table.txt\n",
    "[02:10:50 PM]: OTU table contains 1183 OTUs\n",
    "[02:10:50 PM]: Mapping OTUs to Mock Community (USEARCH)\n",
    "[02:10:50 PM]: Sorting OTU table naturally\n",
    "[02:10:50 PM]: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
    "[02:10:50 PM]: Normalizing OTU table to number of reads per sample\n",
    "[02:10:50 PM]: Index bleed, samples into mock: 1.531092%.\n",
    "[02:10:50 PM]: Will use value of 1.600000% for index-bleed OTU filtering.\n",
    "[02:10:51 PM]: Subtracting 86 from OTU table\n",
    "[02:10:51 PM]: Filtering OTU table down to 685 OTUs\n",
    "[02:10:51 PM]: Filtering valid OTUs\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then assign taxonomy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ufits taxonomy \\\n",
    "-i filtd.final.binary.txt \\\n",
    "-f filtd.filtered.otus.fa \\\n",
    "-d COI \\\n",
    "-o hybrid\n",
    "\n",
    "#note you can also pass the following command to compare output with COI database-only calls\n",
    "ufits taxonomy \\\n",
    "-i filtd.final.binary.txt \\\n",
    "-f filtd.filtered.otus.fa \\\n",
    "-d COI \\\n",
    "-o COIonly \\\n",
    "--method usearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
