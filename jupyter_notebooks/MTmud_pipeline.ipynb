{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Overview\n",
    "\n",
    "The following data was created by extracting soil samples using the MoBio HTP Soil kit, amplifying an arthropod COI fragment using new JOH primers, then 250 PE sequencing on an Illumina HiSeq2500. RunStats for these data can be found [here] [link2]. Note that these 8 samples were pooled with many other samples as part of a single run with several unique projects.\n",
    "\n",
    "See [UFITS Github page] [link1] for overview of install and executing code. Because program continues to be under development, use instructions on README.md file only.\n",
    "\n",
    "This notebook will describe a modification from the traditional UFITS pipeline which uses Robert Edgar's UPARSE algorithm to merge PE reads, cluster OTUs, etc. Instead, we'll use a newly employed algorithm called DADA2 (see [paper] [link3] and [github page] [link4]) which can do the same things as the original UFITS pipeline plus error correct reads. The advantage here is we can likely keep samples which have lower numbers of reads because this algorithm has a way to error correct the Illumina reads prior to clustering the merged PE reads - however Jon Palmer continues to advise against this but in any event this algorithm seems to do a better job of calling what's there and leaving out what's not.\n",
    "\n",
    "[link1]:https://github.com/nextgenusfs/ufits\n",
    "[link3]:http://www.nature.com/articles/nmeth.3869.epdf?author_access_token=hfTtC2mxuI5t44WUhsz05NRgN0jAjWel9jnR3ZoTv0N5gu3rLNk61gF4j2hXPcagLe964qdfd3GRw8OwyZxfEgsul8lwR1lEWykR3lWF30Dl_bZWMvTPwOdrwuiBUeYa\n",
    "[link4]:https://github.com/benjjneb/dada2\n",
    "[link2]:http://cobb.unh.edu/161103_DevonP1_Lane1_A_DemuxStats.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset information\n",
    "\n",
    "Using 'pandas' to import data table shown below. Program already installed via 'pip install pandas' with associated dependencies. See documentation for [pandas here] [link1].  See also [useful pandas code] [link2] (though some may be outdated) and [pandas cookbook] [link3].\n",
    "[link1]:http://pandas.pydata.org/pandas-docs/version/0.18.1/options.html\n",
    "[link2]:http://www.swegler.com/becky/blog/2014/08/06/useful-pandas-snippets/\n",
    "[link3]:http://pandas.pydata.org/pandas-docs/stable/cookbook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          SampleID WellNumber   CoreID SampleNum TopDepth(cm) BottomDepth(cm)                             Description\n",
      "1         mud.9062        A02  CH10_sh         2          0.5               1      no sediment remaining in whirl pak\n",
      "2         mud.9063        A03  CH10_sh         3            1             1.5         sediment remaining in whirl pak\n",
      "3         mud.9064        A04  CH10_sh         4          1.5               2      no sediment remaining in whirl pak\n",
      "4         mud.9065        A05  CH10_sh         5            2             2.5      no sediment remaining in whirl pak\n",
      "5         mud.9066        A06  CH07_sh         1            0             0.5         sediment remaining in whirl pak\n",
      "6         mud.9067        A07  CH07_sh         2          0.5             1.5         sediment remaining in whirl pak\n",
      "7         mud.9068        A08  CH07_sh         3          1.5               2  little sediment remaining in whirl pak\n",
      "8         mud.9069        A09  CH07_sh         4            2             2.5  little sediment remaining in whirl pak\n",
      "0  negunemud.umA01        NaN       na        na           na              na                                      na\n"
     ]
    }
   ],
   "source": [
    "sampleinfo = r'/Users/devonorourke/Documents/Lab.Foster/guano/soil_mt/sampleinfo.csv'\n",
    "df = pd.read_csv(sampleinfo)\n",
    "df.columns = ['SampleID', 'WellNumber', 'CoreID', 'SampleNum', 'TopDepth(cm)', 'BottomDepth(cm)', 'Description']\n",
    "df = df.sort_values(by=\"SampleID\")\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runstats information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             SampleName   Index Yield(MB)     #Reads  %PerfectIndexRead  MeanPhred\n",
      "SampleName Index              Yield(Mb)  #Reads    Q30>=%  MeanPhred                NaN        NaN\n",
      "mud_9062   CGAAGTAT-ACGACGTG         15  58,146     90.04      35.71                NaN        NaN\n",
      "mud_9063   TAGCAGCT-ACGACGTG          1   3,634     92.27      36.55                NaN        NaN\n",
      "mud_9064   TCTCTATG-ACGACGTG          6  23,458     90.45      35.87                NaN        NaN\n",
      "mud_9065   GATCTACG-ACGACGTG          1   2,980     89.99      35.74                NaN        NaN\n",
      "mud_9066   GTAACGAG-ACGACGTG          0     198     85.96       34.4                NaN        NaN\n",
      "mud_9067   ACGTGCGC-ACGACGTG          3  10,610     90.63         36                NaN        NaN\n",
      "mud_9068   ATAGTACC-ACGACGTG          0   1,130     87.41      34.82                NaN        NaN\n",
      "mud_9069   GCGTATAC-ACGACGTG          0     160      82.2       32.9                NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "runstats = r'/Users/devonorourke/Documents/Lab.Foster/guano/soil_mt/mudrunstats.csv'\n",
    "df = pd.read_csv(runstats, \n",
    "                sep = \",\", \n",
    "                names = [\"SampleName\", \"Index\", \"Yield(MB)\", \"#Reads\", \"%PerfectIndexRead\", \"MeanPhred\"])\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running UFITS\n",
    "\n",
    "Running version 0.5.5.  \n",
    "New COI database installed with this version.\n",
    "\n",
    "#install COI db for taxonomy calls if needed\n",
    "ufits install -i COI\n",
    "\n",
    "    #can check to determine which db installed by entering\n",
    "    ufits taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 1 - data cleaning\n",
    "\n",
    "One of the first thing to do with the raw data is move exactly the files you want into a specific directory. For this sequencing run there was more than just MT_mud samples getting sequenced, so you have to parse out the files we want from the files we don't. In this case, it's just a matter of using a wildcard with the SampleName in the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv Project_DevonP1_Lane1/Sample_mud_906*/*.gz ./mudMT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And copy the relevant negative control samples too (we're copying here because these are relevant to another undrelated project too):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cp Project_DevonP1_Lane1/Sample_negunemud_um*/*.gz ./mudMT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will leave behind the .csv files you don't need, and keep all the sample .fastq.gz files in a single directory for UFITS to work with, as it's intended. Once that's complete, you then need to rename the files because that three-part naming schemed used to separate them out in the first place actually doesn't jive with how UFITS scripts want to parse things...It defaults to looking at that first underscore and chopping everything off after that, so samples all get merged together into one big morass, which you don't want at all.  \n",
    "\n",
    "Instead, use the following example to rename the scripts to incorporate a hyphen, which UFITS is okay with:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#note you have to be within the directory structure to run this!\n",
    "#this should both the samples and negative controls as needed.\n",
    "\n",
    "rename 's/mud_/mud\\-/' *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is properly named, you can then proceed.  \n",
    "\n",
    "This process can take a considerable amount of time; increase the number of cpus when possible is advised.  \n",
    "For example, with 8 cpus on 14 samples (containing 17 Gb of data), it took ~1 hour to process. \n",
    "\n",
    "Data stored in various subdirectories within parent directory at: __'/leo/devon/projects/bri2016/ufits_test_data'__.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#trim primers\n",
    "ufits illumina \\\n",
    "-i /leo/devon/projects/mudMT/fastq_files \\\n",
    "-o MTmud \\\n",
    "--read_length 250 \\\n",
    "--rescue_forward \\\n",
    "--require_primer off \\\n",
    "--min_len 160 \\\n",
    "--full_length \\\n",
    "--cpus 8 \\\n",
    "-f GGTCAACAAATCATAAAGATATTGG \\\n",
    "-r GGWACTAATCAATTTCCAAATCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from this script will contain several new files:  \n",
    "\n",
    "1. In the directory in whch the script was executed:\n",
    "    - (output_name).mergedPE.log containing information about PE merging. Note that information in this single file includes the summary of all individual merged pairs.\n",
    "    - (output_name)-filenames.txt containing information about files used in this pipeline (such as index-pair combinations)\n",
    "    - (output_name).demux.fq containing a concatenated file of all trimmed and PE merged sequences of all samples listed in the '-filenames.txt' file above __(this one is to be used in the subsequent OTU clustering step)__\n",
    "\n",
    "2. In the (output_name) directory named in the 'ufits illumina ...' argument provided above:  \n",
    "    - (output_name).ufits-process.log containing information about the demultiplexing and read trimming processes\n",
    "    - a single sample_name.fq file which is the PE merged file from each sample_name...R1/R2 pair of raw fastq inputs\n",
    "    - a single sample_name.demux.fq file for each PE merged sample_name.fq file having been trimmed as defined  \n",
    "\n",
    "Output will also contain a summary of information regarding the total number of reads processed as well as the number of reads processed per sample. Note in this example how there's about a 1,000-fold difference in reads per sample! We won't be using every one of those reads - in fact, we'll likely just use the top 3."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2016-12-22 20:27:19,553: 53,951 reads processed\n",
    "2016-12-22 20:27:19,672: Found 24 barcoded samples\n",
    "                        Sample:  Count\n",
    "                      mud-9062:  28960\n",
    "                      mud-9064:  11691\n",
    "                      mud-9067:  5295\n",
    "                      mud-9063:  1814\n",
    "                      mud-9065:  1482\n",
    "               negunemud-umB12:  1175\n",
    "               negunemud-umA12:  1055\n",
    "                      mud-9068:  544\n",
    "                        ....\n",
    "                      mud-9069:  69\n",
    "               negunemud-umB10:  58\n",
    "               negunemud-umB06:  55\n",
    "               negunemud-umB04:  55\n",
    "               negunemud-umB09:  40\n",
    "               negunemud-umB11:  30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at this point it's valuable to point out that you may want to exclude certain samples which contain too few reads. These can be discarded at this point before moving forward in the OTU clustering part. Jon Palmer recommends shooting for samples with about 50,000 reads, though as low as 10,000 samples are fine. In the example above, you may consider removing all samples except **'mud-9062'**, **'mud-9064'**, and **'mud-9067'** because all other samples contain about as many reads as any of our contaminant samples.  \n",
    "\n",
    "You can either provide a list of samples to keep or to remove - in this instance it's easier to just specify which ones to keep in a list directly in the command line rather than in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufits select \\\n",
    "-i MTmud.demux.fq \\\n",
    "-l mud-9062 mud-9064 mud-9067 \\\n",
    "-o MTmud_cleaned_merged.demux.fq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll generate a new {output.name}.fq file containing only the samples you wanted to keep (or, without all the samples you just dropped). To double check that new .fq file contains just what you want, run the following command to double check you didn't miss anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufits show -i MTmud_cleaned_merged.demux.fq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should produce a list of the barcoded samples that remain; confirm you have the samples you want and don't have samples you intended on discarding. Once that's done it's on to OTU clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "----------------------------------\n",
    "Found 3 barcoded samples\n",
    "              Sample:  Count\n",
    "            mud-9062:  28960\n",
    "            mud-9064:  11691\n",
    "            mud-9067:  5295\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2 - OTU clustering\n",
    "\n",
    "See Jon Palmer's [notes] [link1] about DADA2 if you're curious about what's required to get to this point.  \n",
    "\n",
    "You're going to first have to run a command to get rid of any possible N's in the cleaned, merged, and demux'd fastq file - DADA2 will crash if it detects any nucleotide character other than A/C/G/T:\n",
    "[link1]:https://github.com/nextgenusfs/ufits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vsearch \\\n",
    "-fastq_filter MTmud_cleaned_merged.demux.fq \\\n",
    "-fastq_maxns 0 \\\n",
    "-fastqout MTmud_cleaned_merged_noN.demux.fq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see an output that is really short, and indicates that you dropped a few sequences with those pesky 'N' characters:'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vsearch v2.3.0_linux_x86_64, 251.9GB RAM, 32 cores\n",
    "https://github.com/torognes/vsearch\n",
    "\n",
    "Reading input file 100%  \n",
    "45924 sequences kept (of which 0 truncated), 22 sequences discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those N characters removed, you can run DADA2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ufits dada2 \\\n",
    "--fastq MTmud_cleaned_merged_noN.demux.fq \\\n",
    "--out dada2_output \\\n",
    "--length 180 \\\n",
    "--platform illumina \\\n",
    "--uchime_ref COI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample readout from this command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[08:39:27 PM]: OS: linux2, 32 cores, ~ 264 GB RAM. Python: 2.7.6\n",
    "[08:39:27 PM]: ufits v.0.5.5\n",
    "[08:39:27 PM]: VSEARCH v2.3.0\n",
    "[08:39:27 PM]: Loading FASTQ Records\n",
    "[08:39:27 PM]: 45,924 reads (18.0 MB)\n",
    "[08:39:27 PM]: Quality Filtering, expected errors < 1.0\n",
    "[08:39:28 PM]: 45,753 reads passed\n",
    "[08:39:29 PM]: DADA2 compatible read lengths, avg: 187 bp, min: 160 bp, max: 251 bp, top 95%: 187 bp\n",
    "[08:39:29 PM]: Splitting FASTQ file by Sample and truncating to 180 bp\n",
    "[08:39:30 PM]: Running DADA2 pipeline\n",
    "[08:39:50 PM]: R v3.3.2, DADA2 v\"dada2_output_filtered/mud-9062.fastq\n",
    "[08:39:50 PM]: 84 total inferred sequences (iSeqs)\n",
    "[08:39:50 PM]: 2 denovo chimeras removed\n",
    "[08:39:50 PM]: 82 valid iSeqs\n",
    "[08:39:50 PM]: Chimera Filtering (VSEARCH) using COI DB\n",
    "[08:39:57 PM]: 81 iSeqs passed, 1 ref chimeras removed\n",
    "[08:39:57 PM]: Mapping reads to DADA2 iSeqs\n",
    "[08:40:00 PM]: 45,682 reads mapped to iSeqs (99%)\n",
    "[08:40:00 PM]: Building iSeq OTU table\n",
    "[08:40:01 PM]: Clustering iSeqs at 97% to generate biological OTUs\n",
    "[08:40:01 PM]: 75 OTUs generated\n",
    "[08:40:01 PM]: Mapping reads to OTUs\n",
    "[08:40:03 PM]: 45,675 reads mapped to OTUs (99%)\n",
    "[08:40:03 PM]: Building OTU table\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two files you want to use in the subsequent 'ufits filter' command below:\n",
    "   - **dada2_output.cluster.otus.fa** (use this fasta file in the next command)\n",
    "   - **dada2_output.cluster.otu_table.txt** (use this fasta file in the next command)\n",
    "\n",
    "It also produces two very similar looking files that represent the inferred sequences which were further clustered at 97% identity (so we can make some sort of sense of it when assigning taxonomy) - these aren't necessary for downstream analyses as of now:\n",
    "   - **dada2_output.iSeqs.fa**\n",
    "   - **dada2_output.otu_table.txt**\n",
    "\n",
    "You can see exactly which of those iSeq files ended up clustering together into a single OTU with this file:\n",
    "   - **iSeqs 2 OTUs: dada2_output.iSeqs2clusters.txt**\n",
    "\n",
    "There are also a few log files generated:\n",
    "   - **dada2_output.dada2.Rscript.log**  (to ensure the DADA2 program was run successfully)\n",
    "   - **dada2_output.ufits-dada2.log**  (tracks the overall processing of this wrapper 'ufits dada2' script)\n",
    "\n",
    "\n",
    "Next up is to filter the OTU table with the fasta file listed above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: filtering OTU table\n",
    "\n",
    "See the comments in the 'ufits_standardpipeline_bridata' workflow. Note that all we're using here is the fixed index-bleed workflow, but future analyses should use the mock community data once it becomes available.  We're going to run using the default parameters including the **--min_reads_otu** flag being set to it's default (2)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ufits filter \\\n",
    "-i dada2_output.cluster.otu_table.txt \\\n",
    "-f dada2_output.cluster.otus.fa \\\n",
    "-o dada2_r97m2 \\\n",
    "-p 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't appear that this fixed index removed any potential OTUs:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[08:41:08 PM]: OS: linux2, 32 cores, ~ 264 GB RAM. Python: 2.7.6\n",
    "[08:41:08 PM]: ufits v.0.5.5\n",
    "[08:41:08 PM]: USEARCH v8.1.1831\n",
    "[08:41:08 PM]: Loading OTU table: dada2_output.cluster.otu_table.txt\n",
    "[08:41:08 PM]: OTU table contains 75 OTUs\n",
    "[08:41:08 PM]: Sorting OTU table naturally\n",
    "[08:41:08 PM]: Removing OTUs according to --min_reads_otu: (OTUs with less than 2 reads from all samples)\n",
    "[08:41:08 PM]: Normalizing OTU table to number of reads per sample\n",
    "[08:41:08 PM]: Overwriting auto detect index-bleed, setting to 0.500000%\n",
    "[08:41:08 PM]: Filtering OTU table down to 75 OTUs\n",
    "[08:41:08 PM]: Filtering valid OTUs\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a bunch of new files, but the two relevant ones to use in assigning taxonomy next are:  \n",
    "   - **dada2_r97m2.filtered.otus.fa**\n",
    "   - **dada2_r97m2.final.binary.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Assigning Taxonomy to OTUs\n",
    "\n",
    "Make sure to have already downloaded the necessary database (COI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ufits taxonomy -i dada2_r97m2.final.binary.csv -f dada2_r97m2.filtered.otus.fa -d COI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which produces the following summary (note the OTU classification is performed with UTAX, using a database acquired through BOLD; see J. Palmer for details about its creation):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "[08:42:28 PM]: OS: linux2, 32 cores, ~ 264 GB RAM. Python: 2.7.6\n",
    "[08:42:28 PM]: ufits v.0.5.5\n",
    "[08:42:28 PM]: USEARCH v8.1.1831\n",
    "[08:42:28 PM]: vsearch v2.3.0\n",
    "[08:42:28 PM]: Loading FASTA Records\n",
    "[08:42:28 PM]: 75 OTUs\n",
    "[08:42:28 PM]: Global alignment OTUs with usearch_global (USEARCH)\n",
    "[08:42:33 PM]: Classifying OTUs with UTAX (USEARCH8)\n",
    "[08:42:34 PM]: Appending taxonomy to OTU table and OTUs\n",
    "[08:42:34 PM]: Taxonomy finished: dada2_r97m2.taxonomy.txt\n",
    "[08:42:34 PM]: Classic OTU table with taxonomy: dada2_r97m2.otu_table.taxonomy.txt\n",
    "[08:42:35 PM]: BIOM OTU table created: dada2_r97m2.biom\n",
    "[08:42:35 PM]: OTUs with taxonomy: dada2_r97m2.otus.taxonomy.fa\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we find?\n",
    "\n",
    "See the R script 'ufits_2016_OTUtable_analysis.R' for generating the following observations.  \n",
    "\n",
    "Transfer out the **\"dada2_r97m2.otu_table.taxonomy.txt\"** file with rsync for import into that R script."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#run in directory on local machine where you want the file deposited:\n",
    "rsync devon@pinky.sr.unh.edu:/leo/devon/projects/mudMT/dada2_r97m2.otu_table.taxonomy.txt .\n",
    "\n",
    "#it may also be worth transferring the .fa file with all the taxonomy used:\n",
    "rsync devon@pinky.sr.unh.edu:/leo/devon/projects/mudMT/dada2_r97m2.otus.taxonomy.fa ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLASTing some of those unknowns \n",
    "\n",
    "We can use the following scripts to run NCBI BLAST. This will happen in three steps:\n",
    "\n",
    "- A. Clean up the file to pull out just the UTAX-tagged OTUs and convert into a single-line fasta\n",
    "- B. Build a blast database (* note we're not going to use this step at the moment*)\n",
    "- C. BLAST the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.** First, use fastx toolkit to convert 'failedreads.fasta' into oneliner fasta"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fasta_formatter -i dada2_r97m2.otus.taxonomy.fa -o allOTUs_oneliner.fasta -w 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then grep only lines with \"UTAX\" in header and remove the dashes where the lines were removed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "grep \"UTAX\" allOTUs_oneliner.fasta -A 1 | sed '/^-/d' > utaxOutput_only.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B.** We're going to skip the step of setting up a BLAST database for the moment - there's already the complete 'nr' database installed on Pinky, so unless we want to set up a custom database with specific sequences we don't have available through NCBI, then I wouldn't worry about this step. If you did want to do it, you'd run the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "makeblastdb -in {blast_subjects.fasta} -parse_seqids -dbtype nucl -out {path.to}/my_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C.**. Finally, we're going to run a BLAST search on the UTAX-specific reads using NCBI's nr database. We're going to specify a few other flags described in the code below. Specifically, we're going to take only values that are a certain alignment length, a certain percent identity, and then filter sults these to choose the best possible taxonomic description to those collapsed reads.  \n",
    "\n",
    "See [here] [link1] for BLAST manual with command line options used below.\n",
    "[link1]:http://www.ncbi.nlm.nih.gov/books/NBK279675/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blastn \\\n",
    "-query /leo/devon/projects/mudMT/utaxOutput_only.fa \\\n",
    "-db /opt/ncbi-blast-2.2.29+/db/nt \\\n",
    "-outfmt '6 qseqid sseqid pident length bitscore evalue staxids' \\\n",
    "-num_threads 8 \\\n",
    "-perc_identity 79.9 \\\n",
    "-max_target_seqs 10 \\\n",
    "-out /leo/devon/projects/mudMT/blastout/rd1out.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
