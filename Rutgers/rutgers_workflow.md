# Background
Single guano samples failed to generate significant numbers of amplicons; batch samples were substituted to improve the proportion of DNA extracted in hopes that more amplicons could be produced per (batch) sample. DNA extraction was performed by combining ~ 3-5 guano pieces per sample from each batch. Because we had excess guano per extraction, two separate extractions were performed from the same batch.

Initial amplification of extracted DNA was as poor as individual samples. It is likely that there is extensive amount of DNA degradation in these dry samples; the addition of more tissue (guano) should increase the likelihood of template DNA target to amplify, however it also increase likelihood of inhibitors to PCR. To increase concentration and reduce inhibitors, each sample was subject to a 0.4x SPRI bead cleanup. The replicate samples were then pooled together and new amplification was conducted. Samples showed improved concentration of amplicons, though primer dimers were still extensive. Rather than removing these dimers on a per-sample basis with another SPRI bead cleanup (and risking lose even more amplicon target), samples were normalized (pooled) using total DNA (of PCR products); the subsequent pool was then filtered using the QiaQuick PCR cleanup spin column. The library was then submitted to Northern Arizona University's sequencing center.

## sequencing at NAU
The pooled library of COI amplicons were sequenced using a MiSeq platform following 250 bp PE sequencing using V2 chemistry set for 500 cycles Northern Arizona University's sequencing center on January 17, 2017. Raw numbers of reads and general run metrics are available to view from [this Google Sheet](https://docs.google.com/spreadsheets/d/1uW1KONV-eMAPNyBe0-6iJSIvnG1MYa3nmCaTDsG5i68/edit#gid=0). Less the fraction of reads dedicated to the positive control, about 6.3 million reads were generated across 104 negative control and true samples (negative controls contributed just 1.45% of these reads).

## file naming

The initial names applied to the **.fastq** files automatically generated by NAU were simplified. The output from NAU for a file followed one of two naming conventions:

- `CONTROL-{SampleName}-xx-EN-USA-2017-076-JF_S{###}_L001_R1_001.fastq.gz` for mock community (positive) and negative control samples, with `SampleName` being specific to each sample, and `###` being either a two or three digit value assigned to the NAU sample sheet (effectively a redundant sample name they apply to each sample; shared with each forward and reverse **.fastq** file pair)
- `NHCS-{SampleName}-xx-VE-USA-2017-076-JF_S{###}_L001_R2_001.fastq.gz` for all true samples, following the same naming scheme as described above

The goal was to produce file names with the following scheme: `{SampleName}_{barcode}_L001_R{#}_001.fastq.gz`.

A series of steps were applied to achieve that, as [described here](https://github.com/devonorourke/guano/blob/master/rutgers/renaming_scheme.md).  

# amptk pipeline

[amptk](https://github.com/nextgenusfs/amptk) is a bioinformatic toolkit which performs all necessary tasks beginning with quality and adapter trimming of raw reads, clustering OTUs, denoising and chimera detection, through to assigning taxonomy to each identified cluster and generating (among other outputs) the list of per-sample taxa represented in the dataset. A full documentation of available parameters used for the program are [detailed here](http://amptk.readthedocs.io/en/latest/index.html)

> A note about versions - in addition to the core Python scripts comprising `amptk`, several dependencies were also installed. Versions used in this analysis include:
- amptk v. 1.0.3
- usearch v. 9.2.64
- vsearch v. 2.6.2
- remaining python modules and R dependencies were installed via Conda

## adapter trimming and PE merging

The first step in the pipeline trims adapters (as a result of the insert length being less than the read length) and then uses USEARCH to merge paired end reads. Orphaned reads are discarded (this typically accounts for less then 2% of the overall number or reads in a sample). A single **.fastq.gz** file is output by concatenating all the individual paired reads with headers modified to specify the sample name. In addition, the **.amptk-demux.log** file documents the proportion of merged reads per sample. These ranged from the high end (~3.5 million; positive control) to just 26 total reads for a single sample. There was a significant distribution of numbers of reads for true samples which reflects the stochasticity inherent in amplifying targets from guano extracts, challenges in properly quantifying amplicon vs. primer dimer when pooling, and preference for pure DNA over extract (positive control vs. all else).  

>amptk initially failed because files weren't decompressed properly. This was resolved and appended at the very first step of the renaming process described above. In addition, a `filt_fq` directory was created prior to executing the command, and the script was designed to dump the output files into that directory. The entire script was submitted using our SLURM job submission software; pertinent information regarding the amptk-specific arguments were as follows:  

```
amptk illumina \
-i /mnt/lustre/macmaneslab/devon/guano/NAU/p8-2/raw_fq \
-o trim \
--rescue_forward on \
--require_primer off \
--min_len 160 \
--full_length \
--read_length 250 \
-f GGTCAACAAATCATAAAGATATTGG \
-r GGWACTAATCAATTTCCAAATCC \
--cpus 24 \
--cleanup
```

## dropping samples

There is an important tradeoff between the likelihood that a read is the result of index bleed versus a true representation of the amplicons in a sample; if one is to account and filter for index-bleed, then one is to likely reduce the number of reads in a sample. In addition, because the mock community proportion of reads was large in this run, the likelihood of index bleed is higher than if the positive control had a moderate number of reads (proportional to per-sample read numbers). There are additional filtering parameters applied to account for this, though these filtering parameters work best when the low-read number samples are discarded. The following code was executed to drop the samples with less than 4800 reads, as that value represented samples with less than 0.5% total reads indexed to this project on the lane.

```
amptk remove \
-i /mnt/lustre/macmaneslab/devon/guano/NAU/p8-2/filt_fq/trim.demux.fq.gz \
-t 4800 \
-o dropd.demux.fq

gzip dropd.demux.fq
```

This is a significant threshold as it drops 37 of 104 possible samples. However, the total number of reads which are excluded from the dataset is just 1% of the overall amount. A separate design in which all samples, or a lower threshold read-number applied, is also possible to carry through and compare if major difference arise in OTU abundance, index bleed thresholds, etc. As a first approximation, this more conservative approach seemed most sensible. Analysis of this reduced dataset has the suffix `dropd` applied, while a dataset in which all samples are included are termed `trimd`.  

## clustering for OTUs

This is a two step process in which the **.fastq** file containing all reads is parsed first using the `DADA2` algorithm creating **iseq** candidate sequences (unique sequences which are not clustered), then these unique sequences are clustered to a 97% similarity using the more traditional `UCLUST` approach. See Jon's documentation describing the differences [here](http://amptk.readthedocs.io/en/latest/clustering.html). In brief, **iseq** values are clustered at a 100% identity, whereas the resulting **OTUs** are clustered at 97% identity, meaning that the **iseq** sequences are more exclusive than the **OTUs**.  

In addition there is a chimera filtering step applied to the data; this requires the installation of the COI database provided by amptk:

```
amptk install -i COI
```

Then execute the clustering with the following code:

```
amptk dada2 \
--fastq dropd.demux.fq.gz \
--out dropd \
--length 180 \
--platform illumina \
--uchime_ref COI
```

Note that both `dropd` and `trimd` datasets had the same code applied. The output contains a pair of files which are applied in the next filtering strategy (for index bleed): the `.cluster.otu_table.txt` file which follows a traditional OTU matrix format, as well as the accompanying `.cluster.otus.fa` file which contains the OTU id in the header and the associated sequence. Each dataset is then filtered according to the following commands.  

## filtering

Because a mock community was added to this project, the proportion of reads that are likely misassigned can be estimated on a per-OTU basis. In brief, we are certain of the OTUs likely to be present in mock community; any additional OTU is the result of index bleed. By calculating the proportion of reads that are present in our mock sample which _shouldn't be there_ we can estimate what fraction of reads (on a per-OTU basis) should be subtracted from all true samples.

This process takes place by applying an initial filtering step that filters reads using the most strict criteria (taking the largest instance of an OTU bleed and applying that percentage to filter across true samples); intermediate files are kept to investigate how the index-bleed is distributed on a per-OTU basis. I have maintained a [separate document](https://github.com/devonorourke/guano/blob/master/Rutgers/filtering_notes.md) describing the detailed steps used to apply what I feel are the most appropriate filtering strategies for this dataset. In addition, data output from these filtering steps is contained in a [supplementary spreadsheet](https://docs.google.com/spreadsheets/d/1OQVuGjC5trpjTDsATPYikvr6J0B_bCLT1yoJc7i8NzQ/edit#gid=1765686535).  

In brief, this amounted to determining the proportion of index bleed _into the mock community_, the proportion of index bleed _from mock community into true samples_, and identifying any OTUs which were clustered yet not identified in the mock fasta file. Following data filtering, an initial dataset containing **953 OTUs** and **9,335,697 reads** is reduced to **442 OTUs** and **5,626,434 reads**. A pair of output files after completing filtering steps are applied to then assign taxonomic information to our remaining reads.  

## taxonomy assignment
As described in the [amptk taxonomy](http://amptk.readthedocs.io/en/latest/taxonomy.html) section, the database used to assign taxonomy is derived from the Barcode of Life Database ([BOLD](http://v4.boldsystems.org/)). The sequences present in the database we're using are the result of two sequential clustering processes.
- BOLD's BIN data serve as the initial sequence material. These sequences themselves are initially derived from [a clustering process](http://v4.boldsystems.org/index.php/Public_BarcodeIndexNumber_Home).
- The BIN sequences are then clustered locally by amptk to 99% identity. These data are further processed to train the UTAX program which can be used in taxonomic assignment/prediction.  

> This database was updated as of 14-sept-2017, following the [release](https://github.com/nextgenusfs/amptk/releases/tag/1.0.0) of amptk v-1.0.0.  
> Complete database download is [available here](https://osf.io/4xd9r/files/)

Taxonomy was explored using both the _hybrid_ approach (default in amptk) as well as a _usearch only_ approach. In both instances, the `--method` reflected the given approach. See Jon's description of the steps used in his documentation at the link above.  

The following code was applied (in this example the method is the `hybrid` approach):  

```
amptk taxonomy \
-i /mnt/lustre/macmaneslab/devon/guano/NAU/p8-2/filt/filt_final/fullFilt.final.binary.txt \
--fasta /mnt/lustre/macmaneslab/devon/guano/NAU/p8-2/filt/filt_final/fullFilt.filtered.otus.fa \
--out rut16_h \
--db COI \
--method usearch \
--mapping_file /mnt/lustre/macmaneslab/devon/guano/NAU/p8-2/illumina/dropd.mapping_file.txt
```

 An R script was then used to manipulate the output `rut16_h.otu_table.taxonomy.txt` file which includes both further data filtering, as well as the calculations for frequency tables and visualizations.
