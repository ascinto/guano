{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample background \n",
    "\n",
    "The following sequence data was generated either DNA extracted following guano extraction standards developed in the Foster Lab. Arthropod COI fragments were produced using new JOH primers, then 250 PE sequencing on an Illumina HiSeq2500 generated our reads.  A single lane was used in generating these data, multiple projects were pooled together on this lane. Each project from this single land was independently analyzed to identify potential chimeric sequences, tag-jumping instances, and any consistent sources of cross contmination.  In addition, though these samples were subject to two independent sequencing runs (as the first run did not generate many reads per sample), only reads acquired through this single latter round of sequencing are reflected in these analyses. Run statistics are indicated [here] [link1]. \n",
    "\n",
    "[link1]:http://cobb.unh.edu/170125_DevonP2L1_A_DemuxStats.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Overview\n",
    "\n",
    "We use the tools developed by Jon Palmer in the [**amptk**] [link1] program which will trim, filter, cluster, and assign taxonomy to reads. The core clustering process employs an algorithm called DADA2 (see [paper] [link3] and [github page] [link4]) which has an added benefit of error correcting reads (even singletons), though each identified cluster is then reclustered at a 97% similarity (as done conventionally in many OTU-clustering approaches) for downstream community analyses. Taxonomic assignment is completed in part using the curated Barcode of Life Database [(BOLD)] [link7], while also leveraging Robert Edgar's [SINTAX] [link5] and [UTAX] [link6] algorithms to provide additional taxonomic information for reads which are not matched in BOLD.\n",
    "\n",
    "[link1]:https://github.com/nextgenusfs/amptk\n",
    "[link3]:http://www.nature.com/articles/nmeth.3869.epdf?author_access_token=hfTtC2mxuI5t44WUhsz05NRgN0jAjWel9jnR3ZoTv0N5gu3rLNk61gF4j2hXPcagLe964qdfd3GRw8OwyZxfEgsul8lwR1lEWykR3lWF30Dl_bZWMvTPwOdrwuiBUeYa\n",
    "[link4]:https://github.com/benjjneb/dada2\n",
    "[link5]:http://www.biorxiv.org/content/early/2016/09/09/074161\n",
    "[link6]:http://www.drive5.com/usearch/manual/utax_algo.html\n",
    "[link7]:http://v4.boldsystems.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other information\n",
    "\n",
    "Installation requirements are found on the [amptk] [link1] site. This project was completed with the following dependencies:\n",
    "\n",
    "- amptk v0.8.5 \n",
    "- vsearch v2.3.4\n",
    "- usearch v9.2.64 (linked as \"usearch9\", not \"usearch\")  \n",
    "\n",
    "In addition to the basic tools we're also going to need to have installed a few other items:  \n",
    "- a fasta file of the mock community (used in the sequence index bleed filtering step)\n",
    "    - we are using the IM3 mock community provided by Michelle Jusino\n",
    "- the COI index (either default from 'amptk' or a curated one of your own)\n",
    "\n",
    "To install the amptk default COI database:\n",
    "\n",
    "[link1]:https://github.com/nextgenusfs/amptk/blob/master/docs/ubuntu_install.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk install COI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1 - data migration and cleaning\n",
    "\n",
    "Initial steps involve moving, renaming, and filtering data as follows.\n",
    "\n",
    "First, move all relevant directories from Cobb to Pinky servers with rsync. For each project, there will likely be three sets of samples to migrate: \n",
    "- the samples\n",
    "- the negative controls\n",
    "- one or more mock community members  \n",
    "\n",
    "This will occur for every lane for every project. In instances where there are multiple lanes in which samples were split across, each lane's data will be treated with separate mock community samples so that index bleed is lane-specific. Data for an entire project is only merged after OTUs have been subsequently determined; this is done in R.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsync -avzr foster@cobb.unh.edu:/path2data/Sample_*/*.gz /copy2here/fqRaw/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic renaming\n",
    "\n",
    "Rename the files to remove the unnecessary underscore. As there are several different prefixes to deal with, we'll run this command as many times as needed for each unique prefix type (while implementing wildcards when possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for true samples\n",
    "rename 's/{name}_/{name}\\-/' *\n",
    "\n",
    "#for any contaminant sample\n",
    "rename 's/{name}-contaminated_/{name}une-contaminated/' *\n",
    "\n",
    "#for negative controls\n",
    "rename 's/neg{name}_/negu{name}\\-/' *\n",
    "\n",
    "#for mock commmunity\n",
    "rename 's/mock_IM3_/mockIM3\\-/' *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is properly named, you can then proceed. If you get errors in downstream processes circle back and see if you've kept any weird underscores or other characters in the file names. See the [amptk homepage] [link1] if you're unclear about how the sequence data is expected to be named.\n",
    "\n",
    "[link1]:https://github.com/nextgenusfs/amptk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One final pre-processing note\n",
    "\n",
    "If you have any files that contain zero bytes of data (literally no indices were detected during the Illumina run), you need to delete those .fq files ahead of time before trimming with the first step in the 'amptk' pipeline (it crashes when it tries to parse data that doesn't exist!). To delete files with zero bytes of data, enter the following command (within the directory containing all the raw fastq files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " find . -name \"*.fastq\" -size -1c -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read processing\n",
    "\n",
    "This process can take a considerable amount of time; increase the number of cpus when possible is advised. For example, with 8 cpus on about 17 Gb of data, it took ~1 hour to process.  \n",
    "\n",
    "Note the suffix \"L1\" in the **-o** flag. This is done in this particular circumstance because this project contains data across two lanes. If there were multiple Illumina runs, these would be further specified by project number (ex. P1L1, P3L2, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run in parent directory of {path.to}/fqRaw\n",
    "amptk illumina \\\n",
    "-i /leo/devon/projects/guano/mudMT/p2_data/fqRaw \\\n",
    "-o mud \\\n",
    "--rescue_forward on \\\n",
    "--require_primer off \\\n",
    "--min_len 160 \\\n",
    "--full_length \\\n",
    "--cpus 20 \\\n",
    "--read_length 250 \\\n",
    "-f GGTCAACAAATCATAAAGATATTGG \\\n",
    "-r GGWACTAATCAATTTCCAAATCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from this script will contain several new files:  \n",
    "\n",
    "1. In the directory in whch the script was executed:\n",
    "    - (output_name).mergedPE.log containing information about PE merging. Note that information in this single file includes the summary of all individual merged pairs.\n",
    "    - (output_name)-filenames.txt containing information about files used in this pipeline (such as index-pair combinations)\n",
    "    - (output_name).demux.fq containing a concatenated file of all trimmed and PE merged sequences of all samples listed in the '-filenames.txt' file above __(this one is to be used in the subsequent OTU clustering step)__\n",
    "\n",
    "2. In the (output_name) directory named in the 'ufits illumina ...' argument provided above:  \n",
    "    - (output_name).ufits-process.log containing information about the demultiplexing and read trimming processes\n",
    "    - a single sample_name.fq file which is the PE merged file from each sample_name...R1/R2 pair of raw fastq inputs\n",
    "    - a single sample_name.demux.fq file for each PE merged sample_name.fq file having been trimmed as defined  \n",
    "\n",
    "The file labeled **'{-o name}.ufits-demux.log'** will also contain a summary of information regarding the total number of reads processed as well as the number of reads processed per sample (at the very bottom of the file). This is critical in evaluating which reads to keep, and the names of those files are essential in the next step for keeping/dropping samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping/keeping certain samples\n",
    "\n",
    "You may want to exclude certain samples which contain too few reads. These can be discarded at this point before moving forward in the OTU clustering part. Jon Palmer recommends shooting for samples with about 50,000 reads, though as low as 10,000 samples are fine. It also depends on the distribution of the reads across all samples; if you have 20 samples with >100,000 reads each, and 20 samples with ~10,000 you might not want to keep anything less than 10,000 reads. However, if you have 20 samples with ~20,000 reads each and another 10 samples with 5,000 reads, and another 10 samples with ~100 reads, then you might want to keep those with 5,000 (so going lower than the 10,000 reads threshold).\n",
    "\n",
    "There's an equivalent set of commands which you can specify to keep or remove samples; if it's faster to enter just a few samples to drop, us the '**amptk remove**' command, if it's faster to just specify the few samples you want to keep, use the '**amptk select**' command.  \n",
    "\n",
    "In this specific run with just 9 samples we see a distribution like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sample:  Count\n",
    "mockIM3:  554502\n",
    "mud-9064:  292799\n",
    "mud-9062:  73355\n",
    "mud-9067:  60846\n",
    "mud-9063:  26222\n",
    "mud-9065:  22458\n",
    "mud-9068:  4400\n",
    "mud-9066:  1974\n",
    "mud-9069:  54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we clearly don't want to keep that last sample, **mud-9069**, and the question is what to do about the other two low samples. One of the things to check out are other negative controls from the run, but there weren't any specific to this round of sequencing for these samples; however there were other negative controls pooled in this same lane and they generally topped out around ~1500 reads. In other words, we likely want to drop the bottom two samples total, and keeping that third from last sample, **mud-9068** is probably not a great idea if you're doing community analyses and need to compare between samples, but in this project we're just trying to figure out what's possibly in there of interest. If we find something interesting that's unique to **mud-9068** that might be a bit suspicious, but in general we should be able to keep it in there to get a sense of consistency of OTUs among all samples to find trends.  \n",
    "\n",
    "To remove those last two samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk remove \\\n",
    "-i mud.demux.fq \\\n",
    "-l mud-9066 mud-9069 \\\n",
    "-o mud_filt.demux.fq\n",
    "    #kept 1034582 of 1036610 (99.8%) of all reads\n",
    "    #kept 6 of 8 samples mud samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To double check that new .fq file contains just what you want, run the following command to double check you didn't miss anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk show -i {newly.named}.demux.fq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're good to start with the next part - clustering OTUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 2 - OTU clustering\n",
    "\n",
    "See Jon Palmer's [notes] [link1] about DADA2 if you're curious about what's required to get to this point.  \n",
    "\n",
    "We used to have to clean up reads before passing into DADA2 using the vsearch program, but now the code in amptk deals with this problem just fine.\n",
    "[link1]:https://github.com/nextgenusfs/ufits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with just IM2 remaining (dropped IM4 and mockIM3)\n",
    "amptk dada2 \\\n",
    "--fastq mud_filt.demux.fq \\\n",
    "--out mud \\\n",
    "--length 180 \\\n",
    "--platform illumina \\\n",
    "--uchime_ref COI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output highlights include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "- 1,022,415 reads passed\n",
    "- 282 total inferred sequences (iSeqs)\n",
    "- 81 denovo chimeras removed\n",
    "- 201 valid iSeqs\n",
    "- Chimera Filtering (VSEARCH) using COI DB\n",
    "- 198 iSeqs passed, 3 ref chimeras removed\n",
    "- 1,027,131 reads mapped to iSeqs (99%)\n",
    "- 151 OTUs generated\n",
    "- 1,022,800 reads mapped to OTUs (99%)\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two files you want to use in the subsequent 'ufits filter' command below:\n",
    "   - **{name}.cluster.otus.fa** (use this fasta file in the next command)\n",
    "   - **{name}.cluster.otu_table.txt** (use this fasta file in the next command)\n",
    "\n",
    "It also produces two very similar looking files that represent the inferred sequences which were further clustered at 97% identity (so we can make some sort of sense of it when assigning taxonomy) - these aren't necessary for downstream analyses as of now:\n",
    "   - **{name}.iSeqs.fa**\n",
    "   - **{name}.otu_table.txt**\n",
    "\n",
    "You can see exactly which of those iSeq files ended up clustering together into a single OTU with this file:\n",
    "   - **iSeqs 2 OTUs: {name}.iSeqs2clusters.txt**\n",
    "\n",
    "There are also a few log files generated:\n",
    "   - **{name}.dada2.Rscript.log**  (to ensure the DADA2 program was run successfully)\n",
    "   - **{name}.ufits-dada2.log**  (tracks the overall processing of this wrapper 'ufits dada2' script)\n",
    "\n",
    "\n",
    "Next up is to filter the OTU table with the fasta file listed above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: filtering OTU table\n",
    "\n",
    "It's advocated by Jon Palmer to use an index-bleed filter command using the '-b mock' flag, specifying the use of a mock community to calculate the index-bleed percentage. The idea behind this filtering step is to identify the number of instances in which a read mapped to an OTU which is *not supposed to be in the mock community* is found in the mock community. The percentage represents the overall number of reads that map to mock OTUs (ie. that are supposed to be in there) relative to the number of reads from OTUs that aren't. The alternative approach is to just trim down reads by some defined (yet arbitrary) percent across all samples, given some other examples in other data sets. I don't like that because I've found that every data set is unique, so having a mock community to judge this by is better.  \n",
    "\n",
    "We're going to use our mock community to calculate index bleed. We're also going to (potentially) incorporate a subtraction value in which *if* the scenario occurs such that an unwanted OTU remains in the mock community following the application of the index-bleed filter, we can detect how many reads that maximum value should be and subtract from there to ensure that **zero** normalized reads remain in the highest 'bleeding' OTU in the mock sample. This value is subtracted from *all* reads from *all* samples per OTU, not just the mock community, so it ultimately drops a lot of OTUs with low read thresholds.  \n",
    "\n",
    "There are intermediate files which are very useful in determing exactly how many read of which OTU are creeping into your mock community (that are unwanted OTUs), so you'll notice we're passing a **\"--debug\"** flag which is used to generate these files; without that flag, you'll miss the normalized read counts used in calculating the index bleed filter as well as the subsequent \"--subtract\" value.\n",
    "\n",
    "You might want to play with the **--subtract** threshold and examine how many overall OTUs are retained. In the two examples below, we're going to play with the most conservative approach (filt1) versus a slightly less harsh filter (filt2).  \n",
    "- **firstfilt** uses two flags: the index-bleed flag, and the **--subtract auto** flag. This second flag will calculate what the total number of normalized reads are in OTUs unwated in the mock community and then subtract that value from all reads. This results a completely clean mock, but drops a lot of samples\n",
    "- **mud** uses the same two flags, but produces a different output format than the default csv and removes the mock community information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First example allows for an automatic detection and application of index-bleed and OTU subtraction (if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk filter \\\n",
    "-i mud.cluster.otu_table.txt \\\n",
    "-f mud.cluster.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--keep_mock \\\n",
    "--calculate in \\\n",
    "--mc /leo/devon/projects/guano/mock-fa/CFMR_insect_mock3.fa \\\n",
    "--debug \\\n",
    "--subtract auto \\\n",
    "-o firstfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting output will show you that:\n",
    "- there was an index bleed of **1.4%** identified (that's pretty high!) representing the proportion of an unwanted OTU from another samples bleeding into the mock (it's the worst case scenario, so it's the highest possible instance). \n",
    "- the auto subtract filter was used at a level of **1250**; that's a huge value which needs to be addressed.  \n",
    "\n",
    "In other words after filtering down all reads by 1.4% on a per-OTU basis, all reads were then subtracted form each sample's OTU by an additional 1250 reads. This results in dropping the total number of OTUs present in the table from **151 OTUs** down to just **36 OTUs**.  \n",
    "\n",
    "The next consideration is whether or not the OTUs could be kept if we were imposing a less stringent standard. If you run the same command (above) without passing the **--subtract auto** flag, you'll keep nearly all original OTUs. This indicates that it's this **--subtract** flag that's dropping most of our information.  \n",
    "\n",
    "To investigate what's going on we're going check out the **{name}.normalized.num.txt** file and compare it with the **.final.txt** file. We need to figure out which OTUs in the mock community have the greater number of OTUs that are *unwanted* - that is, OTUs which should not be in the mock community. We're going to clean up the output of the needed files just a bit so that our search basic linux tools work properly (this is because the space in the first line of the files shifts the first row into one more field than all the rows below).\n",
    "\n",
    "Run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sed -i 's/#OTU ID/OTUid/' firstfilt.final.txt\n",
    "sed -i 's/#OTU ID/OTUid/' firstfilt.normalized.num.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't know two things at this point: \n",
    "- how many fields are there?\n",
    "- which OTUs contain the most reads among OTUs that **don't** belong?  \n",
    "\n",
    "To answer the *number of fields question*, substitute **file.name.txt** with whatever file you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "awk -F '\\t' '{print NF; exit}' file.name.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "awk -F '\\t' '{print NF; exit}' firstfilt.final.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result indicates that there are **8** fields (or samples). That's important when you want to next figure out which columns to sort. With our current data set the second field is the mock community, and the first field is the list of OTUs in each sample. So we're going to just sort through this text file and  print out the fields we want.  \n",
    "\n",
    "The following command should give you a sense of how many reads may need to be subtracted from a given OTU that's unwanted. In the first command you can view the OTU in question, the mock sample, and another real sample in terms of how many reads are in each OTU.  \n",
    "- Notice how there is a complete separation between reads that are in the mock and *not in the real sample* and vice versa. This is a good thing.\n",
    "- Notice in the second command how once we've applied those filters that there is zero background noise in our real sample, yet all of our mock community members are maintained (there are 32 unique 'MockIM' samples, and there are 34 listed in our fasta, except that twice we have two samples where they will for a single OTU cluster becuase they are just variants sequences of the same species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for the .normalized.num.txt file:\n",
    "cut firstfilt.normalized.num.txt -f 1,2,3 | sort -k2,2n | awk '$2 != \"0.0\" {print $0}' \n",
    "\n",
    "#for the .final.txt file:\n",
    "cut firstfilt.final.txt -f 1,2,3 | sort -k2,2n | awk '$2 != \"0\" {print $0}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what do you find? There's a single OTU contaminating these samples. It's likely a contaminant present in the PCR mix or the DNA extraction materials because it's present in both the mock community as well as some real samples.  \n",
    "\n",
    "Fortunately, it's the only *unwanted* OTU present in the mock community in significant numbers of reads. Thus, by dropping that single OTU across all samples should help alleviate contamination concerns.  \n",
    "\n",
    "It's valuable to think about what that OTU may represent. If, for example, you wanted to see how many reads are in every sample of the data set for some specific OTU, say  **OTU16**, just type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grep \"\\\\bOTU16\\\\b\" firstfilt.normalized.num.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will result in the following output:\n",
    "\n",
    "**OTU60&nbsp;&nbsp;  1250.0&nbsp;  271.0&nbsp;&nbsp;  1488.0&nbsp;&nbsp;  38.0&nbsp;&nbsp;  0.0&nbsp;&nbsp;  3.0&nbsp;&nbsp;  0.0&nbsp;&nbsp;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful in instances in which there are one or two OTUs that clearly are unwanted but are much higher in background noise than all the others. This can result in dropping a lot of real samples at the expense of a few OTUs. This is often an indication of index-bleed, as chimeric sequences would typically how low read abundance in many samples (the opposite of what we see above). But that's just not what's going on with this dataset... things look pretty normal. You can run a similar *grep* command for each OTU of concern and you'll notice that each case gives the same impression of likely index-bleed.  \n",
    "\n",
    "Another useful thing to check is identifying (as best as possible) the taxonomic information of these potential contaminant OTUs in question. They may be chimeric or index-bleed and this is another way to identify what's going on. You can do this by looking into the fasta file for each of those OTUs and then manually BLASTing them. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grep \"OTU16\" mud.cluster.otus.fa -A 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces the sequence needed; manually BLASTing this specific OTU in NCBI's nr database shows that it's a common housefly. It's expected in other sequences in projects that were spiked in together with this project (that is, this run consisted of not only these 8 mud samples, but also hundreds of other bird and bat fecal samples... and birds eat flies). Thus it's not a chimerica OTU, though it's possible that the fly contaminant OTU is the result of index bleed from other non-mud sample reads present in a handful of samples. However, in this case it's clearly not somewithing we can say *should* be in the dataset, and it's the only OTU behaving so differently from all other reads, so we're going to just drop it.  \n",
    "\n",
    "The final filter is then applied by using the \"auto-subtract\" function again, but only after dropping the specific contaminant OTU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk drop \\\n",
    "-i mud.cluster.otus.fa \\\n",
    "-r mud_filt.demux.fq \\\n",
    "-l OTU16 \\\n",
    "-o mud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command will result in a single OTU being dropped from each .fq file and generate two output files:  \n",
    "- **mud.cleaned.otu_table.txt** \n",
    "- **mud.cleaned.otus.fa**  \n",
    "\n",
    "Next up is to run another filtering step to ensure that the OTU in question has been removed, and see how the removal of that OTU influences what the **--subtract** value is set to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk filter \\\n",
    "-i mud.cleaned.otu_table.txt \\\n",
    "-f mud.cleaned.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--keep_mock \\\n",
    "--calculate in \\\n",
    "--mc /leo/devon/projects/guano/mock-fa/CFMR_insect_mock3.fa \\\n",
    "--subtract auto \\\n",
    "--debug \\\n",
    "-o lasttest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big difference. We find that dropping that one OTU results in the lowering of the **--subtract auto** value from the very high range to a more modest number of reads per sample: from **1250** to **17** reads.\n",
    "\n",
    "While there are still a few additional samples bleeding into the mock community these all occur at a very low abundnace. We can therefore apply a final filtering step where we keep only true samples (we drop the mock community and it's uniquely associated OTUs), and filter down with a more modest subraction filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk filter \\\n",
    "-i mud.cleaned.otu_table.txt \\\n",
    "-f mud.cleaned.otus.fa \\\n",
    "-b mockIM3 \\\n",
    "--calculate in \\\n",
    "--mc /leo/devon/projects/guano/mock-fa/CFMR_insect_mock3.fa \\\n",
    "--subtract auto \\\n",
    "-o mud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we see? By dropping just that one contaminant OTU from each data set we greatly reduce the value used in the **--subtract auto** option. We go from a subract value of **1250** to just **17**; this results in retaining **117** OTUs instead of the **36** that were left when we kept that one contaminant OTU. Big difference!\n",
    "\n",
    "Notably, 11 of those OTUs are specifically associated with the mock community that is now removed. When you look at the abundance of reads per OTU it's pretty clear that many of these dropped OTUs are very rare (are present only in one or a few of the samples and absent in others), yet often when we see OTUs containing thousdands of reads they tend to be present in many of the samples. \n",
    "\n",
    "The arguments above will produce the following files:  \n",
    "- **mud.filtered.otus.fa** is the final filtered fasta file (say that five times fast)\n",
    "- **mud.final.binary.txt** is the presence/absence OTU table after filtering\n",
    "- **mud.stats.txt** is the number of OTUs in each sample before and after filtering\n",
    "- **mud.final.txt** is the OTU table with normalized read counts (noramlized to the number of reads in each sample)\n",
    "- **mux.amptk-filter.log**  \n",
    "\n",
    "The next step is to assign taxonomy by using the **mud.filtered.otus.fa** and **mud.final.binary.txt** files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Assigning Taxonomy to OTUs\n",
    "\n",
    "Make sure to have already downloaded the necessary database (COI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amptk taxonomy -i mud.final.binary.txt -f mud.filtered.otus.fa -d COI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we find?\n",
    "\n",
    "See email and GitHub page for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
