# Introduction
All information created for this project is available at [this Github repo](https://github.com/devonorourke/guano/tree/master/OahuBird). Please visit that page for more information regarding data tables, visualizations, and code used to complete this work.

## molecular work
> Alissa Scinto will update this extraction history...  

- include any information regarding the experimental design in which collections were performed (ie. what areas were the fecal samples collected, when were they collected, by whom were they collected, etc.)  
- include description of how samples were collected; what media they were stored in (ex. ethanol, air, RNAlater, etc.); whether they were frozen; how they were shipped
- include specific kit and protocols used to extract DNA (ex. MoBio? Qiagen? list any deviations from manufacturer protocol)  

- Provide details of PCR work; were all samples amplified in one round? are there negative or posiive controls? how were (amplicon) samples quantified? what target concentration were samples pooled? were samples primer dimers removed - if so, how?  

## sequencing at NAU
The two pooled libraries of COI amplicons were sequenced using a MiSeq platform following 300 bp PE sequencing using V3 chemistry set for 600 cycles Northern Arizona University's sequencing center on February 10, 2018 (p10-1) and February 25, 2018 (p10-2). Raw numbers of reads and general run metrics are available to view for libraries [p10-1](https://docs.google.com/spreadsheets/d/1nXT6GJNZ3Gy4PHRYa8s9mExXZpY2Y2ayoZTteQsvvsU/edit#gid=0) and [p10-2](https://docs.google.com/spreadsheets/d/1ZDOYiIGTKL7_cP8gFDDGn78mRlCs8z-NIUwzzwcnTRw/edit#gid=0). A total of 9,219,224 and 10,719,628 reads were sequenced for p10-1 and p10-2 libraries, respectively.  

## file naming

The initial names applied to the **.fastq** files automatically generated by NAU were simplified. The output from NAU for a file followed one of two naming conventions:

- `CONTROL-{SampleName}-xx-EN-USA-2017-076-JF_S{###}_L001_R1_001.fastq.gz` for mock community (positive) and negative control samples, with `SampleName` being specific to each sample, and `###` being either a two or three digit value assigned to the NAU sample sheet (effectively a redundant sample name they apply to each sample; shared with each forward and reverse **.fastq** file pair)
- `NHCS-{SampleName}-xx-VE-USA-2017-076-JF_S{###}_L001_R2_001.fastq.gz` for all true samples, following the same naming scheme as described above

The goal was to produce file names with the following scheme: `{SampleName}_{barcode}_L001_R{#}_001.fastq.gz`.

A series of steps were applied to achieve that, as [described here](https://github.com/devonorourke/guano/blob/master/OahuBird/docs/renaming.md).  

# amptk pipeline

[amptk](https://github.com/nextgenusfs/amptk) is a bioinformatic toolkit which performs all necessary tasks beginning with quality and adapter trimming of raw reads, clustering OTUs, denoising and chimera detection, through to assigning taxonomy to each identified cluster and generating (among other outputs) the list of per-sample taxa represented in the dataset. A full documentation of available parameters used for the program are [detailed here](http://amptk.readthedocs.io/en/latest/index.html).  

Because there are two libraries for this project, initial scripts in this workflow were executed for each library - this is because the index-bleed calculations and potential for contamination are evaluated on a per-library basis. We will ultimately pass individual `amptk illumina`, `amptk cluster`, `amptk filter` and `amptk drop` commands for each library (p10-1 and p10-2); following the final filtering criteria, the two libraries are concatenated and the resulting filtered reads are then re-clustered, filtered, and taxonomy is assigned with the `amptk taxonomy` command.    

## software version history, installation, and virtual environment parameters
A virtual environment was created when completing the installation process. _Recall that `amptk` is written in Python2, not Python3_, thus `conda create -n amptk_env python=3.6` was specified.  

Initial installation proceeded as described in Jon's suggested [installation guide](http://amptk.readthedocs.io/en/latest/#install).  
> A note about versions - in addition to the core Python scripts comprising `amptk`, several dependencies were also installed. Versions used in this analysis include:
- amptk v. 1.1.0
- usearch9 v9.2.64_i86linux32
- usearch10 v10.0.240_i86linux32
- vsearch v2.6.2_linux_x86_64
- remaining python modules and R dependencies were installed via Conda (upgrade/updates with `pip` and/or `conda` performed 5-Mar-2017); install commands were:  

Installation of Python and R dependencies were performed within the virtual environment created:  
```
module purge
module load anaconda/colsa
source activate amptk_env

pip install -U -I biopython natsort pandas numpy matplotlib seaborn edlib biom-format psutil
conda install r-base bioconductor-dada2
conda install r-base bioconductor-dada2
```  

## scripts
While this documentation outlines highlights the core commands used within the `amptk` pipeline, the specific scripts used to execute these commands are saved within [the scripts directory](https://github.com/devonorourke/guano/tree/master/OahuBird/scripts) and named for the `amptk` process in which they were involved. For example, the first script used to apply taxonomy to the data is called `taxonomy.sh`.  

## adapter trimming and PE merging

The first step in the pipeline trims adapters (as a result of the insert length being less than the read length) and then uses USEARCH to merge paired end reads. Orphaned reads are discarded (this typically accounts for less then 2% of the overall number or reads in a sample). A single **.fastq.gz** file is output by concatenating all the individual paired reads with headers modified to specify the sample name. In addition, the **.amptk-demux.log** file documents the proportion of merged reads per sample. These ranged from over a million in the positive controls (ex. ~1.6 million in p10-1 and ~3 million in p10-2) to just 1 read for a single sample (which is ultimately discarded from analysis). There was a significant distribution of numbers of reads for true samples which reflects the stochasticity inherent in amplifying targets from guano extracts, challenges in properly quantifying amplicon vs. primer dimer when pooling, and preference for pure DNA over extract (positive control vs. all else).  

>amptk initially failed because files weren't decompressed properly. This is resolved with the following command:

```
gzip -d *.gz
```

> In addition, an `illumina` directory was created prior to executing the command, and the script was designed to dump the output files into that directory. The entire script was submitted using our SLURM job submission software; pertinent information regarding the amptk-specific arguments were as follows:  

```
amptk illumina \
-i {path/to/input/directory} \
-o trim \
--rescue_forward on \
--require_primer off \
--min_len 160 \
--full_length \
--read_length 300 \
-f GGTCAACAAATCATAAAGATATTGG \
-r GGWACTAATCAATTTCCAAATCC \
--cpus 24 \
--cleanup
```

## clustering for OTUs

This is a two step process in which the **.fastq** file containing all reads is parsed first using the `DADA2` algorithm creating **iseq** candidate sequences (unique sequences which are not clustered), then these unique sequences are clustered to a 97% similarity using the more traditional `UCLUST` approach. See Jon's documentation describing the differences [here](http://amptk.readthedocs.io/en/latest/clustering.html). In brief, **iseq** values are clustered at a 100% identity, whereas the resulting **OTUs** are clustered at 97% identity, meaning that the **iseq** sequences are more exclusive than the **OTUs**.  

In addition there is a chimera filtering step applied to the data; this requires the installation of the COI database provided by amptk:

```
amptk install -i COI
```

Then execute the clustering with the following code:

> there was a decompresson bug with our system and amptk v1.1.0; I manually decompressed the `.demux.fq.gz` file prior to executing the following code (note `amptk` generates a gzipped file following the completion of `amptk illumina` script above)  

```
amptk dada2 \
--fastq {filePrefix}.demux.fq \
--out {filePrefix} \
--length 180 \
--platform illumina \
--uchime_ref COI
```

The output contains a pair of files which are applied in the next filtering strategy (for index bleed): the `.cluster.otu_table.txt` file which follows a traditional OTU matrix format, as well as the accompanying `.cluster.otus.fa` file which contains the OTU id in the header and the associated sequence. Each dataset is then filtered according to the following commands.  

**START HERE**

|  | trim | dropd |
| --- | --- | --- |
| # OTUs clustered | 1,912 | 1,792 |
| # reads mapped to OTUs | 3,543,867 | 3,472,563 |
| # iSeqs clustered | 3,525 | 3,110 |
| # reads mapped to iSeqs | 3,549,960 | 3,478,881 |


## filtering

Because a mock community was added to this project, the proportion of reads that are likely misassigned can be estimated on a per-OTU basis. In brief, we are certain of the OTUs likely to be present in mock community; any additional OTU is the result of index bleed. By calculating the proportion of reads that are present in our mock sample which _shouldn't be there_ we can estimate what fraction of reads (on a per-OTU basis) should be subtracted from all true samples.

This process takes place by applying an initial filtering step that filters reads using the most strict criteria (taking the largest instance of an OTU bleed and applying that percentage to filter across true samples); intermediate files are kept to investigate how the index-bleed is distributed on a per-OTU basis. I have maintained a [separate document](https://github.com/devonorourke/guano/blob/master/Perlut/docs/Perlut_filtering_notes.md) describing the detailed steps used to apply what I feel are the most appropriate filtering strategies for this dataset.   

In brief, this amounted to determining the proportion of index bleed _into the mock community_, the proportion of index bleed _from mock community into true samples_, and identifying any OTUs which were clustered yet not identified in the mock fasta file.  

The filtering results from the `dropd` and `trimd` datasets are as follows:  
- **For `trim`**, the initial dataset retained all **89** total samples, **3,543,867** reads (though > 1,245,00 were from the mock community alone), and **1912 OTUs**. Following filtering, we retained **2,198,389 reads** and **1,094 OTUs** (notably all mock reads have been removed).  
- **For `dropd`** the initial dataset was reduced to just **38** samples (including the mock community) prior to filtering, with **3,472,564 reads** retained among **1,792 OTUs**. Following filtering the dataset included **2,194,756 reads** and **1,254 OTUs**.     

A pair of output files after completing filtering steps are applied to then assign taxonomic information to our remaining reads. Following these steps, the `trim` dataset was used exclusively to assign taxonomy to the OTUs present.  

## taxonomy assignment
As described in the [amptk taxonomy](http://amptk.readthedocs.io/en/latest/taxonomy.html) section, the database used to assign taxonomy is derived from the Barcode of Life Database ([BOLD](http://v4.boldsystems.org/)). The sequences present in the database we're using are the result of two sequential clustering processes.
- BOLD's BIN data serve as the initial sequence material. These sequences themselves are initially derived from [a clustering process](http://v4.boldsystems.org/index.php/Public_BarcodeIndexNumber_Home).
- The BIN sequences are then clustered locally by amptk to 99% identity. These data are further processed to train the UTAX program which can be used in taxonomic assignment/prediction.  

> This database was updated as of 14-sept-2017, following the [release](https://github.com/nextgenusfs/amptk/releases/tag/1.0.0) of amptk v-1.0.0.  
> Complete database download is [available here](https://osf.io/4xd9r/files/)

Taxonomy was explored using both the _hybrid_ approach (default in amptk) as well as a _usearch only_ approach. In both instances, the `--method` reflected the given approach. See Jon's description of the steps used in his documentation at the link above.  

The following code was applied (in this example the method is the `usearch` approach):  

```
amptk taxonomy \
--i /mnt/lustre/macmaneslab/devon/guano/NAU/Perlut/filtd/finaltrim.final.txt \
--fasta /mnt/lustre/macmaneslab/devon/guano/NAU/Perlut/filtd/finaltrim.filtered.otus.fa \
--out Perlut_u \
--db COI \
--method usearch \
--mapping_file /mnt/lustre/macmaneslab/devon/guano/NAU/Perlut/illumina/dropd.mapping_file.txt
```

The output fasta sequence and OTU table with taxonomic information were uploaded to [the `/data/amptk` directory within this Github repo](https://github.com/devonorourke/guano/tree/master/Perlut/data/amptk).  

> Note that a value of 0 ("absence") could mean a variety of different things:  
> - it could be that the OTU is not truly in the sample of guano
> - it could be because an OTU was present but not amplified and sequenced  
> - it could be the OTU was sequenced but there wasn't enough reads to pass our filters (with `--index_bleed` and `--subtract` arguments in `amptk filter`))

 # Further analyses

 An R script was then used to manipulate the output `Perlut.otu_table.taxonomy.txt` file which includes both further data filtering, as well as the calculations for frequency tables and visualizations - [see here](https://github.com/devonorourke/guano/blob/master/Perlut/docs/OTUanalysis.R).  

 > One such data filtering taking place here is the removal of reads associated with the mock community.

Output from this R script are placed within [the `/data/Routput` directory within this Github repo](https://github.com/devonorourke/guano/tree/master/Perlut/data/Routput).  
